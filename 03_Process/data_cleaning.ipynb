{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the separate monthly csvs and put together a dataframe, make \"started_at\" and \"ended_at\" columns datetimes to get a \"ride_length\" in minutes column. Also create \"day of week\" (0=monday), \"start_hour\", \"season\", and \"month\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.abspath(\"\")\n",
    "csv_folder = \"/\".join(dir.split(\"\\\\\")[:-1] + [\"02_Prepare\", \"CSVs\", \"*\"])\n",
    "\n",
    "csv_list = []\n",
    "for csv in glob.glob(csv_folder):\n",
    "    csv_list.append(pd.read_csv(csv, index_col=None, header=0))\n",
    "\n",
    "df = pd.concat(csv_list, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "f = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "Y = 2000  # Dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [('winter', (date(Y,  1,  1),  date(Y,  3, 20))),\n",
    "           ('spring', (date(Y,  3, 21),  date(Y,  6, 20))),\n",
    "           ('summer', (date(Y,  6, 21),  date(Y,  9, 22))),\n",
    "           ('autumn', (date(Y,  9, 23),  date(Y, 12, 20))),\n",
    "           ('winter', (date(Y, 12, 21),  date(Y, 12, 31)))]\n",
    "\n",
    "\n",
    "def get_season(now):\n",
    "    \"\"\"Return current nothern hemisphere season\"\"\"\n",
    "    if isinstance(now, datetime):\n",
    "        now = now.date()\n",
    "    now = now.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= now <= end)\n",
    "\n",
    "df['started_at'] = df['started_at'].apply(f)\n",
    "df['ended_at'] = df['ended_at'].apply(f)\n",
    "df['ride_length'] = df['ended_at'] - df['started_at']\n",
    "df['ride_length'] = df['ride_length'].apply(lambda x: x.total_seconds() / 60)\n",
    "df['day_of_week'] = df['started_at'].apply(lambda x: x.weekday())\n",
    "df[\"start_hour\"] = df[\"started_at\"].apply(lambda x: datetime.strftime(x, \"%H:00\"))\n",
    "df[\"season\"] = df[\"started_at\"].apply(lambda x: get_season(x))\n",
    "df[\"month\"] = df[\"started_at\"].apply(lambda x: datetime.strftime(x, \"%m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how many null values each column has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = df[df.isnull().any(axis=1)]\n",
    "for column in null.columns:\n",
    "    print(f\"{column}: {null[column].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was found that every end_lat that had NaN also had NaN for end_station_name and end_station_id, so no way to recover that data. this only made up ~4800 rows out of 5.75 million so i dropped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df[\"end_station_name\"].isna()) & (df[\"end_lat\"].isna())].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dict with station names as keys and (lat, long) coords as values. also create a dict with coords as keys and station names as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = df.start_station_name.value_counts().index.tolist()\n",
    "stations_coords = {}\n",
    "\n",
    "for station in stations:\n",
    "    t = df.loc[df.start_station_name == station]\n",
    "    stations_coords[station] = (\n",
    "        t.iloc[0, 8],\n",
    "        t.iloc[0, 9]\n",
    "    )\n",
    "\n",
    "reverse_coords = {values: keys for keys, values in stations_coords.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates start_coords column and end_coords column.  \n",
    "uses defined functions to choose the closest station coords based on dicts for rows with NaN for station name(s).  \n",
    "drops redundant columns, makes all lats and longs for the same for each station observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_between_points(x, point):\n",
    "    \"\"\"Returns distance between x and point using pythag\"\"\"\n",
    "    return np.sqrt((point[0]-x[0])**2 + (point[1]-x[1])**2)\n",
    "\n",
    "def minimize(x, point_l):\n",
    "    \"\"\"Returns point from point_l (point list) that x is closest to\"\"\"\n",
    "    lst = np.array([d_between_points(x, i) for i in point_l])\n",
    "    idx = lst.argmin()\n",
    "\n",
    "    return point_l[idx]\n",
    "\n",
    "\n",
    "df[\"start_coords\"] = list(zip(df[\"start_lat\"], df[\"start_lng\"]))\n",
    "df[\"end_coords\"] = list(zip(df[\"end_lat\"], df[\"end_lng\"]))\n",
    "\n",
    "possible_coords = list(reverse_coords.keys())\n",
    "\n",
    "df.loc[df[\"start_station_name\"].isna(), \"start_station_name\"] = df.loc[df[\"start_station_name\"].isna(), \"start_coords\"].apply(lambda x: reverse_coords[minimize(x, point_l=possible_coords)])\n",
    "df.loc[df[\"end_station_name\"].isna(), \"end_station_name\"] = df.loc[df[\"end_station_name\"].isna(), \"end_coords\"].apply(lambda x: reverse_coords[minimize(x, point_l=possible_coords)])\n",
    "\n",
    "df.drop([\"start_station_id\", \"end_station_id\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df[\"start_coords\"] = df[\"start_station_name\"].apply(lambda x: stations_coords[x])\n",
    "df[\"end_coords\"] = df[\"end_station_name\"].apply(lambda x: stations_coords[x])\n",
    "\n",
    "df[\"start_lat\"] = df[\"start_coords\"].apply(lambda x: x[0])\n",
    "df[\"start_lng\"] = df[\"start_coords\"].apply(lambda x: x[1])\n",
    "df[\"end_lat\"] = df[\"end_coords\"].apply(lambda x: x[0])\n",
    "df[\"end_lng\"] = df[\"end_coords\"].apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "df.drop([\"start_coords\", \"end_coords\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swaps started_at and ended_at for observations with negative ride length (only 140 so i prob could've just ignored but either way it won't make much of a difference.)  \n",
    "recomputes ride_length as opposed to just multiplying by -1 bc I felt like it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df[\"ride_length\"] < 0\n",
    "df.loc[idx, [\"started_at\", \"ended_at\"]] = df.loc[idx, [\"ended_at\", \"started_at\"]].values\n",
    "\n",
    "df['ride_length'] = df['ended_at'] - df['started_at']\n",
    "df['ride_length'] = df['ride_length'].apply(lambda x: x.total_seconds() / 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saves dataframe to csv for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Clean_Cyclistic_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d45edfa029243f1b8ef4e844982c6953fc65f9dbbb981b30f79653022d49b3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
